{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T16:59:30.448224Z",
     "start_time": "2023-10-18T16:59:21.472888Z"
    }
   },
   "source": [
    "!pip install transformers  \n",
    "установим spaCy (run in terminal/prompt)  \n",
    "!{sys.executable} -m pip install spacy\n",
    "Загрузим spaCy's  'en' Model  \n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Загрузка-данных\" data-toc-modified-id=\"Загрузка-данных-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Загрузка данных</a></span></li><li><span><a href=\"#Подготовка-функций\" data-toc-modified-id=\"Подготовка-функций-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Подготовка функций</a></span></li><li><span><a href=\"#Подготовка-данных-для-моделей\" data-toc-modified-id=\"Подготовка-данных-для-моделей-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Подготовка данных для моделей</a></span><ul class=\"toc-item\"><li><span><a href=\"#Модели-с-векторизацией-Bag-of-words-и-TF-IDF\" data-toc-modified-id=\"Модели-с-векторизацией-Bag-of-words-и-TF-IDF-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Модели с векторизацией Bag of words и TF-IDF</a></span></li><li><span><a href=\"#Модели-с-векторизацией-BERT\" data-toc-modified-id=\"Модели-с-векторизацией-BERT-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Модели с векторизацией BERT</a></span></li></ul></li></ul></li><li><span><a href=\"#Обучение-и-выбор-моделей\" data-toc-modified-id=\"Обучение-и-выбор-моделей-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение и выбор моделей</a></span><ul class=\"toc-item\"><li><span><a href=\"#Векторизация-Bag-of-words\" data-toc-modified-id=\"Векторизация-Bag-of-words-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Векторизация Bag of words</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия-(bag-of-words)\" data-toc-modified-id=\"Логистическая-регрессия-(bag-of-words)-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Логистическая регрессия (bag of words)</a></span></li><li><span><a href=\"#Модель-на-основе-случайного-леса-(bag-of-words)\" data-toc-modified-id=\"Модель-на-основе-случайного-леса-(bag-of-words)-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Модель на основе случайного леса (bag of words)</a></span></li></ul></li><li><span><a href=\"#Векторизация-TF-IDF-(Term-Frequency–Inverse-Document-Frequency)\" data-toc-modified-id=\"Векторизация-TF-IDF-(Term-Frequency–Inverse-Document-Frequency)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Векторизация TF-IDF (Term Frequency–Inverse Document Frequency)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия-(TF-IDF)\" data-toc-modified-id=\"Логистическая-регрессия-(TF-IDF)-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Логистическая регрессия (TF-IDF)</a></span></li><li><span><a href=\"#Модель-на-основе-случайного-леса-(TF-IDF)\" data-toc-modified-id=\"Модель-на-основе-случайного-леса-(TF-IDF)-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Модель на основе случайного леса (TF-IDF)</a></span></li></ul></li><li><span><a href=\"#Векторизация-BERT\" data-toc-modified-id=\"Векторизация-BERT-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Векторизация BERT</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия-(BERT)\" data-toc-modified-id=\"Логистическая-регрессия-(BERT)-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Логистическая регрессия (BERT)</a></span></li><li><span><a href=\"#Модель-на-основе-случайного-леса-(BERT)\" data-toc-modified-id=\"Модель-на-основе-случайного-леса-(BERT)-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Модель на основе случайного леса (BERT)</a></span></li></ul></li><li><span><a href=\"#Выбор-и-тестирование-модели\" data-toc-modified-id=\"Выбор-и-тестирование-модели-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Выбор и тестирование модели</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация комментариев с BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:26:03.716805Z",
     "start_time": "2023-10-23T20:26:03.457479Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Дмитрий\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Импортируем необходимые библиотеки\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import notebook\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import gc\n",
    "import joblib\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Для лемматизации слов\n",
    "import sys\n",
    "import spacy\n",
    "\n",
    "# Для создания собственных трансформеров\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import (CountVectorizer,\n",
    "                                             TfidfVectorizer)\n",
    "# Масштабирование признаков в разреженных матрицах\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                     GridSearchCV) \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Для работы с BERT\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Константы\n",
    "RANDOM_STATE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:21.833624Z",
     "start_time": "2023-10-22T10:41:15.077658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Загрузим данные\n",
    "try:\n",
    "    df = pd.read_csv('toxic_comments.csv')\n",
    "except:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:21.852893Z",
     "start_time": "2023-10-22T10:41:21.833624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Рассмотрим общие сведения\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:21.868620Z",
     "start_time": "2023-10-22T10:41:21.854896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Рассмотрим первые строки\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные содержат:\n",
    "- столбец `text`с комментариями на английском языке,\n",
    "- столбец `toxic`, содержащий значение 1 для токсичного комментария. **Целевой признак**,\n",
    "- столбец `Unnamed: 0`, не несущий какой-либо информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:21.883633Z",
     "start_time": "2023-10-22T10:41:21.869621Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удалим стобец `Unnamed: 0`.\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:21.913682Z",
     "start_time": "2023-10-22T10:41:21.885122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:21.930354Z",
     "start_time": "2023-10-22T10:41:21.914730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    0.898388\n",
       "1    0.101612\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим балансировку классов\n",
    "df['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеется разбалансировка классов, поэтому в моделях будем применять взвешивание классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:21.947022Z",
     "start_time": "2023-10-22T10:41:21.931398Z"
    }
   },
   "outputs": [],
   "source": [
    "# Комментарии на английском языке, поэтому загрузим стоп-слова для английского языка\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "stop_words = list(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:22.381402Z",
     "start_time": "2023-10-22T10:41:21.948179Z"
    }
   },
   "outputs": [],
   "source": [
    "# Для выполнения лемматизации\n",
    "# инициализируем модель spacy 'для английского, оставив только компонент, необходимый для лемматизации\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:22.396409Z",
     "start_time": "2023-10-22T10:41:22.384451Z"
    }
   },
   "outputs": [],
   "source": [
    "# Определим функцию для предобработки текста\n",
    "def text_preprocessor(text):\n",
    "    # Приведём все символы к нижнему регистру\n",
    "    out_text = text.lower()\n",
    "\n",
    "    # Удалим стоп-слова\n",
    "    out_text = out_text.split()\n",
    "    clean_tokens = tuple(\n",
    "                   map(lambda x: '' if x in stop_words else x, out_text)\n",
    "    )\n",
    "    out_text = ' '.join(clean_tokens)\n",
    "\n",
    "    # Удалим все символы, которые не являются буквами латинского алфавита\n",
    "    out_text = re.sub(r'[^a-zA-Z]', ' ', out_text)\n",
    "\n",
    "    # Выполним лемматизацию\n",
    "    # Parse the sentence using the loaded 'en' model object `nlp`\n",
    "    doc = nlp(out_text)\n",
    "    # Extract the lemma for each token and join\n",
    "    out_text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    # Все одиночные символы, имеющие пробелы с обеих сторон, заменим пробелом\n",
    "    out_text = re.sub(r'\\s+[a-z]\\s+', ' ', out_text)\n",
    "    \n",
    "    # Все последовательности пробелов заменим пробелом\n",
    "    out_text = ' '.join(out_text.split())\n",
    "    \n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:22.415571Z",
     "start_time": "2023-10-22T10:41:22.397932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Определим трансформер для векторизации Bag of words\n",
    "class bow_transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_df):\n",
    "        self.max_df = max_df\n",
    "        self._estimator = CountVectorizer(encoding='utf8',\n",
    "                                          preprocessor=None,\n",
    "                                          lowercase=False,\n",
    "                                          stop_words=None,\n",
    "                                          max_df=self.max_df)\n",
    "    def fit(self, X, y=None):\n",
    "        _ = self._estimator.fit(X['lemm_text'])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_out = self._estimator.transform(X['lemm_text'])\n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:22.433220Z",
     "start_time": "2023-10-22T10:41:22.416571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Определим трансформер для векторизации TF-IDF\n",
    "class tfidf_transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_df, norm):\n",
    "        self.max_df = max_df\n",
    "        self.norm = norm\n",
    "        self._estimator = TfidfVectorizer(encoding='utf8',\n",
    "                                          preprocessor=None,\n",
    "                                          lowercase=False,\n",
    "                                          stop_words=None,\n",
    "                                          max_df=self.max_df,\n",
    "                                          norm=self.norm)\n",
    "    def fit(self, X, y=None):\n",
    "        _ = self._estimator.fit(X['lemm_text'])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_out = self._estimator.transform(X['lemm_text'])\n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:22.448879Z",
     "start_time": "2023-10-22T10:41:22.433882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Масштабирование элементов разряженной матрицы\n",
    "scaler = MaxAbsScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T16:12:33.374859Z",
     "start_time": "2023-10-23T16:12:33.372857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Модель логистической регрессии\n",
    "lr_clf = LogisticRegression(random_state=RANDOM_STATE, solver='saga', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:41:22.483203Z",
     "start_time": "2023-10-22T10:41:22.465543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Модель на основе случайного леса\n",
    "rf_clf = RandomForestClassifier(n_jobs=-1, random_state=RANDOM_STATE, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных для моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модели с векторизацией Bag of words и TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:50:12.080880Z",
     "start_time": "2023-10-22T10:41:22.484140Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 159292/159292 [08:33<00:00, 310.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Подготовка текстов для векторизации Bag of words и TF-IDF\n",
    "df['lemm_text'] = df['text']\n",
    "\n",
    "# Для контроля прогресса будем использовать progress_apply\n",
    "tqdm.pandas()\n",
    "df['lemm_text'] = df['text'].progress_apply(text_preprocessor)\n",
    "\n",
    "# Сохраним результат предобработки текста в файл\n",
    "df.to_csv('comments_with_lemmatized_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:50:12.126102Z",
     "start_time": "2023-10-22T10:50:12.080880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   text       159292 non-null  object\n",
      " 1   toxic      159292 non-null  int64 \n",
      " 2   lemm_text  159292 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Рассмотрим данные\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модели с векторизацией BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем перевести тексты в векторы, подготовим их. У BERT есть собственный токенизатор. Это инструмент, который разбивает и преобразует исходные тексты в список токенов, которые есть в словаре. Лемматизация не требуется.  \n",
    "Для работы возьмём предобученную модель с ресурса https://huggingface.co/unitary/toxic-bert\n",
    "\n",
    "Будем использовать:\n",
    "- модель (файл pytorch_model.bin),\n",
    "- файл конфигурации модели (config.json),\n",
    "- словарь для токенизации (файл vocab.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:53:46.595818Z",
     "start_time": "2023-10-22T10:50:32.005188Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 159292/159292 [03:09<00:00, 841.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Инициализируем токенизатор как объект класса BertTokenizer().\n",
    "# Передадим ему аргумент vocab_file — это файл со словарём, на котором обучалась модель.\n",
    "# Он может быть, например, в текстовом формате (txt)\n",
    "tokenizer = transformers.BertTokenizer(do_lower_case=True,\n",
    "    vocab_file='toxic_bert/vocab.txt')\n",
    "\n",
    "# Преобразуем текст в номера токенов из словаря методом encode()\n",
    "# Для корректной работы модели мы указали аргумент add_special_tokens (англ. «добавить специальные токены»),\n",
    "# равный True. Это значит, что к любому преобразуемому тексту добавляется токен начала (101) и токен конца текста (102). \n",
    "# Для уменьшения количества токенов применим truncation\n",
    "tokenized = df['text'].progress_apply(\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation='longest_first'))\n",
    "\n",
    "# Применим метод padding (англ. «отступ»), чтобы после токенизации длины исходных текстов в корпусе были равными.\n",
    "# Только при таком условии будет работать модель BERT. Пусть стандартной длиной вектора n будет длина наибольшего\n",
    "# во всём датасете вектора. Остальные векторы дополним нулями\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "#Теперь поясним модели, что нули не несут значимой информации. Это нужно для компоненты модели,\n",
    "# которая называется «внимание» (англ. attention). Отбросим эти токены и «создадим маску» для действительно\n",
    "# важных токенов, то есть укажем нулевые и не нулевые значения:\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:54:05.896056Z",
     "start_time": "2023-10-22T10:54:05.880106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159292, 512)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:54:19.805586Z",
     "start_time": "2023-10-22T10:54:19.772971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159292, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:54:24.743863Z",
     "start_time": "2023-10-22T10:54:24.565029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1690"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# освободим оперативную память\n",
    "del tokenized\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T10:55:13.724634Z",
     "start_time": "2023-10-22T10:54:34.076983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Инициализируем саму модель класса BertModel.\n",
    "# Передадим ей файл с предобученной моделью и конфигурацией:\n",
    "config = transformers.BertConfig.from_json_file(\n",
    "    'toxic_bert/config.json')\n",
    "model = transformers.BertModel.from_pretrained(\n",
    "    'toxic_bert/pytorch_model.bin', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем тексты в эмбеддинги. Для наглядного контроля прогресса подключим библиотеку tqdm (араб. taqadum, تقدّم, «прогресс»). В Jupyter применим функцию notebook() из этой библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:01:30.152474Z",
     "start_time": "2023-10-22T17:05:59.523171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a398792c38a54759ab9dabde1b8d1971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving file embeddings_0.csv. indexes = 0:4999\n",
      "saving file embeddings_1.csv. indexes = 5000:9999\n",
      "saving file embeddings_2.csv. indexes = 10000:14999\n",
      "saving file embeddings_3.csv. indexes = 15000:19999\n",
      "saving file embeddings_4.csv. indexes = 20000:24999\n",
      "saving file embeddings_5.csv. indexes = 25000:29999\n",
      "saving file embeddings_6.csv. indexes = 30000:34999\n",
      "saving file embeddings_7.csv. indexes = 35000:39999\n",
      "saving file embeddings_8.csv. indexes = 40000:44999\n",
      "saving file embeddings_9.csv. indexes = 45000:49999\n",
      "saving file embeddings_10.csv. indexes = 50000:54999\n",
      "saving file embeddings_11.csv. indexes = 55000:59999\n",
      "saving file embeddings_12.csv. indexes = 60000:64999\n",
      "saving file embeddings_13.csv. indexes = 65000:69999\n",
      "saving file embeddings_14.csv. indexes = 70000:74999\n",
      "saving file embeddings_15.csv. indexes = 75000:79999\n",
      "saving file embeddings_16.csv. indexes = 80000:84999\n",
      "saving file embeddings_17.csv. indexes = 85000:89999\n",
      "saving file embeddings_18.csv. indexes = 90000:94999\n",
      "saving file embeddings_19.csv. indexes = 95000:99999\n",
      "saving file embeddings_20.csv. indexes = 100000:104999\n",
      "saving file embeddings_21.csv. indexes = 105000:109999\n",
      "saving file embeddings_22.csv. indexes = 110000:114999\n",
      "saving file embeddings_23.csv. indexes = 115000:119999\n",
      "saving file embeddings_24.csv. indexes = 120000:124999\n",
      "saving file embeddings_25.csv. indexes = 125000:129999\n",
      "saving file embeddings_26.csv. indexes = 130000:134999\n",
      "saving file embeddings_27.csv. indexes = 135000:139999\n",
      "saving file embeddings_28.csv. indexes = 140000:144999\n",
      "saving file embeddings_29.csv. indexes = 145000:149999\n",
      "saving file embeddings_30.csv. indexes = 150000:154999\n",
      "saving file embeddings_31.csv. indexes = 155000:159291\n"
     ]
    }
   ],
   "source": [
    "# Эмбеддинги моделью BERT будем создавать батчами. Чтобы хватило оперативной памяти, сделаем размер батча небольшим\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "k = 0\n",
    "# Сделаем цикл по батчам. Отображать прогресс будет функция notebook()\n",
    "for i in notebook.tqdm(range(round(np.ceil(padded.shape[0]/batch_size)))):\n",
    "        # Преобразуем данные в формат тензоров (англ. tensor) — многомерных векторов в библиотеке torch.\n",
    "        # Тип данных LongTensor (англ. «длинный тензор») хранит числа в «длинном формате», то есть выделяет\n",
    "        # на каждое число 64 бита.\n",
    "        # Преобразуем данные\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        # Преобразуем маску\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        # Чтобы получить эмбеддинги для батча, передадим модели данные и маску\n",
    "        # Для ускорения вычисления функцией no_grad() (англ. no gradient, «нет градиента») в\n",
    "        # библиотеке torch укажем, что градиенты не нужны: модель BERT обучать не будем.\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        # Из полученного тензора извлечём нужные элементы (для каждого текста выберем эмбеддинг первого токена,\n",
    "        # соответствующего началу текста) и добавим в список всех эмбеддингов\n",
    "        # преобразуем элементы методом numpy() к типу numpy.array\n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "        # Каждые 5000 текстов будем освобождать оперативную память, сохраняя результат в файл\n",
    "        if len(embeddings) == 50:\n",
    "            print(f'saving file embeddings_{k}.csv. indexes = {k*50*batch_size}:{(k+1)*50*batch_size-1}')\n",
    "            pd.DataFrame(np.concatenate(embeddings)).to_csv(f'embeddings_{k}.csv',\n",
    "                                                            index=False)\n",
    "            k+=1\n",
    "            del embeddings\n",
    "            gc.collect()\n",
    "            embeddings = []\n",
    "print(f'saving file embeddings_{k}.csv. indexes = {k*50*batch_size}:{padded.shape[0]-1}')\n",
    "pd.DataFrame(np.concatenate(embeddings)).to_csv(f'embeddings_{k}.csv',\n",
    "                                                            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:01:30.418718Z",
     "start_time": "2023-10-23T14:01:30.154477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Очистим оперативную память\n",
    "del embeddings\n",
    "del config\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:08:00.517185Z",
     "start_time": "2023-10-23T14:01:30.419718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Соберём эмбеддинги в один файл\n",
    "embeddings = []\n",
    "for i in range(k+1):\n",
    "    embeddings.append(pd.read_csv(f'embeddings_{i}.csv').values)\n",
    "pd.DataFrame(np.concatenate(embeddings)).to_csv(f'embeddings_all.csv',\n",
    "                                                            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:08:00.692558Z",
     "start_time": "2023-10-23T14:08:00.519233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# очистим оперативную память\n",
    "del embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение и выбор моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T16:01:19.071667Z",
     "start_time": "2023-10-23T16:01:09.532658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Загрузим данные\n",
    "df = pd.read_csv('comments_with_lemmatized_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T16:06:56.051930Z",
     "start_time": "2023-10-23T16:06:56.019901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   text       159292 non-null  object\n",
      " 1   toxic      159292 non-null  int64 \n",
      " 2   lemm_text  159269 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Рассмотрим данные\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T16:07:13.103406Z",
     "start_time": "2023-10-23T16:07:13.076382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3983</th>\n",
       "      <td>From here\\n\\nFrom here 160.80.2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4475</th>\n",
       "      <td>1993\\n\\n1994\\n\\n1995\\n\\n1996\\n\\n1997\\n\\n1998\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6289</th>\n",
       "      <td>193.61.111.53  15:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8824</th>\n",
       "      <td>What is I 78.146.102.144</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10193</th>\n",
       "      <td>64.86.141.133\"</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17280</th>\n",
       "      <td>~ \\n\\n68.193.147.157</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23485</th>\n",
       "      <td>between 1991 and 1999</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38743</th>\n",
       "      <td>88.104.31.21\"</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46969</th>\n",
       "      <td>and then 70.8.194.249</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52336</th>\n",
       "      <td>14:53,</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53679</th>\n",
       "      <td>92.24.199.233|92.24.199.233]]</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61644</th>\n",
       "      <td>\"\\n\\n 199.209.144.211  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79643</th>\n",
       "      <td>Some 1 should 216.237.210.43</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82541</th>\n",
       "      <td>\"\\n '''''' 2010/2013 \"</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100596</th>\n",
       "      <td>[24 and 25, after 23]</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119018</th>\n",
       "      <td>\"\"\"</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119697</th>\n",
       "      <td>What 24.119.163.71</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124235</th>\n",
       "      <td>What what what what is this ???</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137400</th>\n",
       "      <td>== \"\"\"</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148595</th>\n",
       "      <td>she did 76.122.79.82</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151107</th>\n",
       "      <td>10 - 2010 04 08 to 2010 05 12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152543</th>\n",
       "      <td>SAME FOR THIS 166.137.240.20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153041</th>\n",
       "      <td>which is OVER 9000 OVER 9000 OVER 9000 OVER 90...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic lemm_text\n",
       "3983                    From here\\n\\nFrom here 160.80.2.8      0       NaN\n",
       "4475    1993\\n\\n1994\\n\\n1995\\n\\n1996\\n\\n1997\\n\\n1998\\n...      0       NaN\n",
       "6289                                 193.61.111.53  15:00      0       NaN\n",
       "8824                             What is I 78.146.102.144      0       NaN\n",
       "10193                                      64.86.141.133\"      0       NaN\n",
       "17280                                ~ \\n\\n68.193.147.157      0       NaN\n",
       "23485                               between 1991 and 1999      0       NaN\n",
       "38743                                       88.104.31.21\"      0       NaN\n",
       "46969                               and then 70.8.194.249      0       NaN\n",
       "52336                                              14:53,      0       NaN\n",
       "53679                       92.24.199.233|92.24.199.233]]      0       NaN\n",
       "61644                            \"\\n\\n 199.209.144.211  \"      0       NaN\n",
       "79643                        Some 1 should 216.237.210.43      0       NaN\n",
       "82541                              \"\\n '''''' 2010/2013 \"      0       NaN\n",
       "100596                              [24 and 25, after 23]      0       NaN\n",
       "119018                                                \"\"\"      1       NaN\n",
       "119697                                 What 24.119.163.71      0       NaN\n",
       "124235                    What what what what is this ???      0       NaN\n",
       "137400                                             == \"\"\"      0       NaN\n",
       "148595                               she did 76.122.79.82      0       NaN\n",
       "151107                      10 - 2010 04 08 to 2010 05 12      0       NaN\n",
       "152543                       SAME FOR THIS 166.137.240.20      0       NaN\n",
       "153041  which is OVER 9000 OVER 9000 OVER 9000 OVER 90...      0       NaN"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Рассмотрим пропуски\n",
    "df[df['lemm_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T16:09:16.297744Z",
     "start_time": "2023-10-23T16:09:16.260591Z"
    }
   },
   "outputs": [],
   "source": [
    "# пропуски заполним значением необработанного текста, добавив слово text\n",
    "df.loc[df['lemm_text'].isna(), 'lemm_text'] = 'text' + df.loc[df['lemm_text'].isna(), 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T16:09:23.751447Z",
     "start_time": "2023-10-23T16:09:23.716198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   text       159292 non-null  object\n",
      " 1   toxic      159292 non-null  int64 \n",
      " 2   lemm_text  159292 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T16:09:29.122913Z",
     "start_time": "2023-10-23T16:09:29.107900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Разделим признаки\n",
    "X = pd.DataFrame(df['lemm_text'].copy(), columns=['lemm_text'])\n",
    "y = df['toxic'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T16:09:31.365622Z",
     "start_time": "2023-10-23T16:09:31.323914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Разделим выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T16:09:32.739890Z",
     "start_time": "2023-10-23T16:09:32.732885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка 127433 (0.80), тестовая выборка 31859 (0.20)\n"
     ]
    }
   ],
   "source": [
    "print(f'Обучающая выборка {X_train.shape[0]} ({X_train.shape[0]/X.shape[0]:.2f}), \\\n",
    "тестовая выборка {X_test.shape[0]} ({X_test.shape[0]/X.shape[0]:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T16:09:37.402625Z",
     "start_time": "2023-10-23T16:09:37.212890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7728"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Очистим оперативную память\n",
    "del df\n",
    "del X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия (bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T22:37:17.463735Z",
     "start_time": "2023-10-23T22:00:37.187648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 60 candidates, totalling 240 fits\n",
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   4.5s\n",
      "[Pipeline] ........... (step 2 of 3) Processing scaling, total=   0.1s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total= 1.1min\n",
      "best_score: 0.543321815153645\n",
      "best_params: {'clf__C': 6, 'clf__max_iter': 2000, 'clf__penalty': 'l2', 'vect__max_df': 2}\n"
     ]
    }
   ],
   "source": [
    "lr_bow_pl = Pipeline(steps=[('vect', bow_transformer(max_df=1.0)),\n",
    "                            ('scaling', scaler),\n",
    "                            ('clf', lr_clf)], verbose=True)\n",
    "\n",
    "param_grid_lr_bow = [{'vect__max_df': [2, 10, 100],\n",
    "                      'clf__max_iter': [2000],\n",
    "                      'clf__C': range(1, 21),\n",
    "                      'clf__penalty':['l2']\n",
    "                      }\n",
    "                     ]\n",
    "gs_lr_bow_pl = GridSearchCV(\n",
    "                           lr_bow_pl,\n",
    "                           param_grid=param_grid_lr_bow,\n",
    "                           scoring='f1',\n",
    "                           cv=4,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=3,\n",
    "                           error_score='raise'\n",
    ")\n",
    "\n",
    "gs_lr_bow_pl.fit(X_train, y_train)\n",
    "\n",
    "gs_lr_bow_best_score = gs_lr_bow_pl.best_score_\n",
    "gs_lr_bow_best_params = gs_lr_bow_pl.best_params_\n",
    "\n",
    "# лучшее значение f1 на кросс-валидации\n",
    "print(f'best_score: {gs_lr_bow_best_score}')\n",
    "# лучшие гиперпараметры\n",
    "print(f'best_params: {gs_lr_bow_best_params}')\n",
    "# Сохраним модель\n",
    "_ = joblib.dump(gs_lr_bow_pl.best_estimator_, 'gs_lr_bow_pl.pkl', compress=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модель на основе случайного леса (bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T22:55:27.162445Z",
     "start_time": "2023-10-23T22:37:17.463735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 27 candidates, totalling 108 fits\n",
      "[Pipeline] .............. (step 1 of 2) Processing vect, total=   4.6s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total= 1.4min\n",
      "best_score: 0.6050004185533085\n",
      "best_params: {'clf__max_depth': None, 'clf__n_estimators': 100, 'vect__max_df': 2}\n"
     ]
    }
   ],
   "source": [
    "rf_bow_pl = Pipeline(steps=[('vect', bow_transformer(max_df=1.0)),\n",
    "                            ('clf', rf_clf)], verbose=True)\n",
    "\n",
    "param_grid_rf_bow = [{'vect__max_df': [2, 10, 100],\n",
    "                      'clf__n_estimators': [10, 50, 100],\n",
    "                      'clf__max_depth': [5, 20, None]\n",
    "                      }\n",
    "                     ]\n",
    "gs_rf_bow_pl = GridSearchCV(\n",
    "                           rf_bow_pl,\n",
    "                           param_grid=param_grid_rf_bow,\n",
    "                           scoring='f1',\n",
    "                           cv=4,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=3,\n",
    "                           error_score='raise'\n",
    ")\n",
    "\n",
    "gs_rf_bow_pl.fit(X_train, y_train)\n",
    "\n",
    "gs_rf_bow_best_score = gs_rf_bow_pl.best_score_\n",
    "gs_rf_bow_best_params = gs_rf_bow_pl.best_params_\n",
    "\n",
    "# лучшее значение f1 на кросс-валидации\n",
    "print(f'best_score: {gs_rf_bow_best_score}')\n",
    "# лучшие гиперпараметры\n",
    "print(f'best_params: {gs_rf_bow_best_params}')\n",
    "# Сохраним модель\n",
    "_ = joblib.dump(gs_rf_bow_pl.best_estimator_, 'gs_rf_bow_pl.pkl', compress=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация TF-IDF (Term Frequency–Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T23:24:29.365351Z",
     "start_time": "2023-10-23T22:55:27.162445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 120 candidates, totalling 480 fits\n",
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   4.8s\n",
      "[Pipeline] ........... (step 2 of 3) Processing scaling, total=   0.1s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=  15.4s\n",
      "best_score: 0.7614821022533786\n",
      "best_params: {'clf__C': 4, 'clf__max_iter': 2000, 'clf__penalty': 'l2', 'vect__max_df': 2, 'vect__norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "lr_tfidf_pl = Pipeline(steps=[('vect', tfidf_transformer(max_df=1.0, norm='l2')),\n",
    "                              ('scaling', scaler),\n",
    "                              ('clf', lr_clf)], verbose=True)\n",
    "\n",
    "param_grid_lr_tfidf = [{'vect__max_df': [2, 10, 100],\n",
    "                        'vect__norm':['l1', 'l2'],\n",
    "                        'clf__max_iter': [2000],\n",
    "                        'clf__C': range(1, 21),\n",
    "                        'clf__penalty':['l2']\n",
    "                       }\n",
    "                      ]\n",
    "gs_lr_tfidf_pl = GridSearchCV(\n",
    "                            lr_tfidf_pl,\n",
    "                            param_grid=param_grid_lr_tfidf,\n",
    "                            scoring='f1',\n",
    "                            cv=4,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=3,\n",
    "                            error_score='raise'\n",
    ")\n",
    "\n",
    "gs_lr_tfidf_pl.fit(X_train, y_train)\n",
    "\n",
    "gs_lr_tfidf_best_score = gs_lr_tfidf_pl.best_score_\n",
    "gs_lr_tfidf_best_params = gs_lr_tfidf_pl.best_params_\n",
    "\n",
    "# лучшее значение f1 на кросс-валидации\n",
    "print(f'best_score: {gs_lr_tfidf_best_score}')\n",
    "# лучшие гиперпараметры\n",
    "print(f'best_params: {gs_lr_tfidf_best_params}')\n",
    "# Сохраним модель\n",
    "_ = joblib.dump(gs_lr_tfidf_pl.best_estimator_, 'gs_lr_tfidf_pl.pkl', compress=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модель на основе случайного леса (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T23:53:35.511676Z",
     "start_time": "2023-10-23T23:24:29.365351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 54 candidates, totalling 216 fits\n",
      "[Pipeline] .............. (step 1 of 2) Processing vect, total=   4.8s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total= 1.1min\n",
      "best_score: 0.6053570563350497\n",
      "best_params: {'clf__max_depth': None, 'clf__n_estimators': 100, 'vect__max_df': 2, 'vect__norm': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "rf_tfidf_pl = Pipeline(steps=[('vect', tfidf_transformer(max_df=1.0, norm='l2')),\n",
    "                              ('clf', rf_clf)], verbose=True)\n",
    "\n",
    "param_grid_rf_tfidf = [{'vect__max_df': [2, 10, 100],\n",
    "                        'vect__norm':['l1', 'l2'],\n",
    "                        'clf__n_estimators': [10, 50, 100],\n",
    "                        'clf__max_depth': [5, 20, None]\n",
    "                        }\n",
    "                       ]\n",
    "gs_rf_tfidf_pl = GridSearchCV(\n",
    "                            rf_tfidf_pl,\n",
    "                            param_grid=param_grid_rf_tfidf,\n",
    "                            scoring='f1',\n",
    "                            cv=4,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=3,\n",
    "                            error_score='raise'\n",
    ")\n",
    "\n",
    "gs_rf_tfidf_pl.fit(X_train, y_train)\n",
    "\n",
    "gs_rf_tfidf_best_score = gs_rf_tfidf_pl.best_score_\n",
    "gs_rf_tfidf_best_params = gs_rf_tfidf_pl.best_params_\n",
    "\n",
    "# лучшее значение f1 на кросс-валидации\n",
    "print(f'best_score: {gs_rf_tfidf_best_score}')\n",
    "# лучшие гиперпараметры\n",
    "print(f'best_params: {gs_rf_tfidf_best_params}')\n",
    "# Сохраним модель\n",
    "_ = joblib.dump(gs_rf_tfidf_pl.best_estimator_, 'gs_rf_tfidf_pl.pkl', compress=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T23:55:38.725612Z",
     "start_time": "2023-10-23T23:53:35.511676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Загрузим данные и разделим признаки, а потом освободим оперативную память\n",
    "X_bert = pd.read_csv('embeddings_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T23:55:38.853729Z",
     "start_time": "2023-10-23T23:55:38.726614Z"
    }
   },
   "outputs": [],
   "source": [
    "# Для экономии памяти преобразуем данные\n",
    "X_bert = X_bert.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T23:55:39.317168Z",
     "start_time": "2023-10-23T23:55:38.854730Z"
    }
   },
   "outputs": [],
   "source": [
    "# Разделим выборки\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(\n",
    "    X_bert, y, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T23:55:39.334562Z",
     "start_time": "2023-10-23T23:55:39.318938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка 127433 (0.80), тестовая выборка 31859 (0.20)\n"
     ]
    }
   ],
   "source": [
    "print(f'Обучающая выборка {X_train_bert.shape[0]} ({X_train_bert.shape[0]/X_bert.shape[0]:.2f}), \\\n",
    "тестовая выборка {X_test_bert.shape[0]} ({X_test_bert.shape[0]/X_bert.shape[0]:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T23:55:39.553310Z",
     "start_time": "2023-10-23T23:55:39.334562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Очистим оперативную память\n",
    "del X_bert\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T00:23:15.040661Z",
     "start_time": "2023-10-23T23:55:39.553310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n",
      "best_score: 0.9246989833178894\n",
      "best_params: {'C': 7, 'max_iter': 2000, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "param_grid_lr_clf = {'max_iter': [2000],\n",
    "                     'C': range(1, 21),\n",
    "                     'penalty':['l2']\n",
    "                     }\n",
    "gs_lr_clf = GridSearchCV(\n",
    "                            lr_clf,\n",
    "                            param_grid=param_grid_lr_clf,\n",
    "                            scoring='f1',\n",
    "                            cv=4,\n",
    "                            n_jobs=-1,\n",
    "                            pre_dispatch=12, # для экономии оперативной памяти\n",
    "                            verbose=3,\n",
    "                            error_score='raise'\n",
    ")\n",
    "\n",
    "gs_lr_clf.fit(X_train_bert, y_train_bert)\n",
    "\n",
    "gs_lr_clf_best_score = gs_lr_clf.best_score_\n",
    "gs_lr_clf_best_params = gs_lr_clf.best_params_\n",
    "\n",
    "# лучшее значение f1 на кросс-валидации\n",
    "print(f'best_score: {gs_lr_clf_best_score}')\n",
    "# лучшие гиперпараметры\n",
    "print(f'best_params: {gs_lr_clf_best_params}')\n",
    "# Сохраним модель\n",
    "_ = joblib.dump(gs_lr_clf.best_estimator_, 'gs_lr_clf.pkl', compress=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модель на основе случайного леса (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T00:30:18.886478Z",
     "start_time": "2023-10-24T00:23:15.040661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n",
      "best_score: 0.9434044153885639\n",
      "best_params: {'max_depth': 20, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "param_grid_rf_clf = {'n_estimators': [10, 50, 100],\n",
    "                        'max_depth': [5, 20, None]\n",
    "                        }\n",
    "gs_rf_clf = GridSearchCV(\n",
    "                         rf_clf,\n",
    "                         param_grid=param_grid_rf_clf,\n",
    "                         scoring='f1',\n",
    "                         cv=4,\n",
    "                         n_jobs=-1,\n",
    "                         pre_dispatch=12, # для экономии оперативной памяти\n",
    "                         verbose=3,\n",
    "                         error_score='raise'\n",
    ")\n",
    "\n",
    "gs_rf_clf.fit(X_train_bert, y_train_bert)\n",
    "\n",
    "gs_rf_clf_best_score = gs_rf_clf.best_score_\n",
    "gs_rf_clf_best_params = gs_rf_clf.best_params_\n",
    "\n",
    "# лучшее значение f1 на кросс-валидации\n",
    "print(f'best_score: {gs_rf_clf_best_score}')\n",
    "# лучшие гиперпараметры\n",
    "print(f'best_params: {gs_rf_clf_best_params}')\n",
    "# Сохраним модель\n",
    "_ = joblib.dump(gs_rf_clf.best_estimator_, 'gs_rf_clf.pkl', compress=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор и тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучшие результаты показала модель на основе случайного леса (гиперпараметры 'max_depth': 20, 'n_estimators': 100) с векторизацией текстов с использованием модели toxic bert. Проверим качество модели на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T04:27:47.905047Z",
     "start_time": "2023-10-24T04:27:47.808902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_RandomForest_bert_test_f1: 0.949\n"
     ]
    }
   ],
   "source": [
    "rf_bert_predict = gs_rf_clf.best_estimator_.predict(X_test_bert)\n",
    "print(f'best_RandomForest_bert_test_f1: {f1_score(y_test_bert, rf_bert_predict):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе работы выполнено:\n",
    "- загрузка данных,\n",
    "- подготовка исходных текстов для моделей с применением Bag of words, TF-IDF и для модели с применением BERT,\n",
    "- определён способ предобработки текста (создан свой предобработчик),\n",
    "- моделирование на основе логистической регрессии в трёх вариантах (с применением Bag of words, TF-IDF, BERT),\n",
    "- моделирование на основе случайного леса в трёх вариантах (с применением Bag of words, TF-IDF, BERT),\n",
    "- для обработки данных применён пайплайн, включающий трансформер собственной разработки, который позволяет изменять параметры векторизации текстов.\n",
    "\n",
    "\n",
    "В ходе экспериментов удалось добиться максимального значения f1 0.949 на тестовой выборке, что соответствует заданным требованиям (f1 не менее 0.75). Наилучший результат показала модель на основе случайного леса с применением векторизации текстов моделью toxic bert. Гиперпараметры определены по итогам кросс-валидации.\n",
    "\n",
    "\n",
    "**Рекомендуемые настройки для использования:**\n",
    "- векторизация моделью toxic bert,\n",
    "- модель на основе случайного леса (гиперпараметры max_depth: 20, n_estimators: 100, class_weight='balanced')."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2840,
    "start_time": "2023-10-18T08:27:45.041Z"
   },
   {
    "duration": 27,
    "start_time": "2023-10-18T08:28:20.872Z"
   },
   {
    "duration": 5,
    "start_time": "2023-10-18T08:31:04.844Z"
   },
   {
    "duration": 2105,
    "start_time": "2023-10-18T08:31:28.795Z"
   },
   {
    "duration": 11,
    "start_time": "2023-10-18T08:31:47.419Z"
   },
   {
    "duration": 23,
    "start_time": "2023-10-18T08:32:07.043Z"
   },
   {
    "duration": 23,
    "start_time": "2023-10-18T08:33:07.220Z"
   },
   {
    "duration": 6,
    "start_time": "2023-10-18T08:33:08.700Z"
   },
   {
    "duration": 8,
    "start_time": "2023-10-18T08:38:09.567Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-18T08:38:43.711Z"
   },
   {
    "duration": 5,
    "start_time": "2023-10-18T08:39:21.961Z"
   },
   {
    "duration": 146,
    "start_time": "2023-10-18T08:39:48.654Z"
   },
   {
    "duration": 13,
    "start_time": "2023-10-18T08:40:15.137Z"
   },
   {
    "duration": 5,
    "start_time": "2023-10-18T08:40:46.906Z"
   },
   {
    "duration": 801,
    "start_time": "2023-10-18T08:40:46.913Z"
   },
   {
    "duration": 17,
    "start_time": "2023-10-18T08:40:47.715Z"
   },
   {
    "duration": 11,
    "start_time": "2023-10-18T08:40:47.734Z"
   },
   {
    "duration": 9,
    "start_time": "2023-10-18T08:40:47.746Z"
   },
   {
    "duration": 27,
    "start_time": "2023-10-18T08:40:47.756Z"
   },
   {
    "duration": 4877,
    "start_time": "2023-10-18T08:45:29.133Z"
   },
   {
    "duration": 2096,
    "start_time": "2023-10-18T08:45:34.012Z"
   },
   {
    "duration": 24,
    "start_time": "2023-10-18T08:45:36.109Z"
   },
   {
    "duration": 12,
    "start_time": "2023-10-18T08:45:36.134Z"
   },
   {
    "duration": 7,
    "start_time": "2023-10-18T08:45:36.148Z"
   },
   {
    "duration": 43,
    "start_time": "2023-10-18T08:45:55.364Z"
   },
   {
    "duration": 4099,
    "start_time": "2023-10-18T08:46:00.835Z"
   },
   {
    "duration": 2112,
    "start_time": "2023-10-18T08:46:04.936Z"
   },
   {
    "duration": 24,
    "start_time": "2023-10-18T08:46:07.049Z"
   },
   {
    "duration": 12,
    "start_time": "2023-10-18T08:46:07.075Z"
   },
   {
    "duration": 7,
    "start_time": "2023-10-18T08:46:07.089Z"
   },
   {
    "duration": 3373,
    "start_time": "2023-10-18T08:47:25.173Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-18T09:04:34.797Z"
   },
   {
    "duration": 161,
    "start_time": "2023-10-18T09:06:57.633Z"
   },
   {
    "duration": 3305,
    "start_time": "2023-10-18T09:10:16.248Z"
   },
   {
    "duration": 2121,
    "start_time": "2023-10-18T09:10:19.555Z"
   },
   {
    "duration": 23,
    "start_time": "2023-10-18T09:10:21.678Z"
   },
   {
    "duration": 15,
    "start_time": "2023-10-18T09:10:21.703Z"
   },
   {
    "duration": 9,
    "start_time": "2023-10-18T09:10:21.720Z"
   },
   {
    "duration": 31,
    "start_time": "2023-10-18T09:10:21.731Z"
   },
   {
    "duration": 10,
    "start_time": "2023-10-18T09:10:21.763Z"
   },
   {
    "duration": 15,
    "start_time": "2023-10-18T09:10:21.775Z"
   },
   {
    "duration": 5,
    "start_time": "2023-10-18T09:10:21.792Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-18T09:12:35.272Z"
   },
   {
    "duration": 3225,
    "start_time": "2023-10-18T09:14:50.570Z"
   },
   {
    "duration": 801,
    "start_time": "2023-10-18T09:14:53.797Z"
   },
   {
    "duration": 17,
    "start_time": "2023-10-18T09:14:54.600Z"
   },
   {
    "duration": 29,
    "start_time": "2023-10-18T09:14:54.618Z"
   },
   {
    "duration": 8,
    "start_time": "2023-10-18T09:14:54.649Z"
   },
   {
    "duration": 31,
    "start_time": "2023-10-18T09:14:54.659Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-18T09:14:54.691Z"
   },
   {
    "duration": 11,
    "start_time": "2023-10-18T09:14:54.695Z"
   },
   {
    "duration": 7,
    "start_time": "2023-10-18T09:14:54.707Z"
   },
   {
    "duration": 15,
    "start_time": "2023-10-18T09:14:54.715Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-18T09:15:40.848Z"
   },
   {
    "duration": 7,
    "start_time": "2023-10-18T09:16:50.009Z"
   },
   {
    "duration": 6,
    "start_time": "2023-10-18T09:17:23.538Z"
   },
   {
    "duration": 812,
    "start_time": "2023-10-18T09:17:23.546Z"
   },
   {
    "duration": 19,
    "start_time": "2023-10-18T09:17:24.360Z"
   },
   {
    "duration": 6,
    "start_time": "2023-10-18T09:17:24.381Z"
   },
   {
    "duration": 24,
    "start_time": "2023-10-18T09:17:24.388Z"
   },
   {
    "duration": 29,
    "start_time": "2023-10-18T09:17:24.414Z"
   },
   {
    "duration": 8,
    "start_time": "2023-10-18T09:17:24.444Z"
   },
   {
    "duration": 5,
    "start_time": "2023-10-18T09:17:24.454Z"
   },
   {
    "duration": 8,
    "start_time": "2023-10-18T09:17:24.460Z"
   },
   {
    "duration": 21,
    "start_time": "2023-10-18T09:17:24.470Z"
   },
   {
    "duration": 18,
    "start_time": "2023-10-18T09:17:24.493Z"
   },
   {
    "duration": 10,
    "start_time": "2023-10-18T09:17:24.512Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-18T09:17:24.523Z"
   },
   {
    "duration": 5,
    "start_time": "2023-10-18T09:17:24.528Z"
   },
   {
    "duration": 5,
    "start_time": "2023-10-18T09:22:35.336Z"
   },
   {
    "duration": 56,
    "start_time": "2023-10-18T09:22:56.637Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-18T09:23:11.231Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-18T09:24:31.173Z"
   },
   {
    "duration": 3428,
    "start_time": "2023-10-18T09:24:48.160Z"
   },
   {
    "duration": 817,
    "start_time": "2023-10-18T09:24:51.590Z"
   },
   {
    "duration": 47,
    "start_time": "2023-10-18T09:24:52.409Z"
   },
   {
    "duration": 16,
    "start_time": "2023-10-18T09:24:52.458Z"
   },
   {
    "duration": 8,
    "start_time": "2023-10-18T09:24:52.477Z"
   },
   {
    "duration": 34,
    "start_time": "2023-10-18T09:24:52.487Z"
   },
   {
    "duration": 20,
    "start_time": "2023-10-18T09:24:52.522Z"
   },
   {
    "duration": 6,
    "start_time": "2023-10-18T09:24:52.544Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-18T09:24:52.552Z"
   },
   {
    "duration": 19,
    "start_time": "2023-10-18T09:24:52.557Z"
   },
   {
    "duration": 14,
    "start_time": "2023-10-18T09:24:52.578Z"
   },
   {
    "duration": 26,
    "start_time": "2023-10-18T09:24:52.593Z"
   },
   {
    "duration": 11,
    "start_time": "2023-10-18T09:24:52.621Z"
   },
   {
    "duration": 19,
    "start_time": "2023-10-18T09:24:52.635Z"
   },
   {
    "duration": 20,
    "start_time": "2023-10-18T09:24:52.656Z"
   },
   {
    "duration": 78,
    "start_time": "2023-10-18T09:24:52.678Z"
   },
   {
    "duration": 2,
    "start_time": "2023-10-18T09:24:52.758Z"
   },
   {
    "duration": 4172,
    "start_time": "2023-10-18T09:34:58.404Z"
   },
   {
    "duration": 837,
    "start_time": "2023-10-18T09:35:02.579Z"
   },
   {
    "duration": 24,
    "start_time": "2023-10-18T09:35:03.418Z"
   },
   {
    "duration": 16,
    "start_time": "2023-10-18T09:35:03.444Z"
   },
   {
    "duration": 7,
    "start_time": "2023-10-18T09:35:03.463Z"
   },
   {
    "duration": 28,
    "start_time": "2023-10-18T09:35:03.471Z"
   },
   {
    "duration": 10,
    "start_time": "2023-10-18T09:35:03.501Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-18T09:35:03.513Z"
   },
   {
    "duration": 5,
    "start_time": "2023-10-18T09:35:03.519Z"
   },
   {
    "duration": 17,
    "start_time": "2023-10-18T09:35:03.540Z"
   },
   {
    "duration": 5,
    "start_time": "2023-10-18T09:35:03.559Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-18T09:35:03.565Z"
   },
   {
    "duration": 6,
    "start_time": "2023-10-18T09:35:03.570Z"
   },
   {
    "duration": 19,
    "start_time": "2023-10-18T09:35:03.578Z"
   },
   {
    "duration": 7,
    "start_time": "2023-10-18T09:35:03.598Z"
   },
   {
    "duration": 67,
    "start_time": "2023-10-18T09:35:03.607Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-18T09:35:03.676Z"
   },
   {
    "duration": 1333,
    "start_time": "2023-10-18T09:35:35.066Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-18T09:36:07.763Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-18T09:36:32.090Z"
   },
   {
    "duration": 25,
    "start_time": "2023-10-18T09:37:29.236Z"
   },
   {
    "duration": 14,
    "start_time": "2023-10-18T09:37:55.142Z"
   },
   {
    "duration": 9,
    "start_time": "2023-10-18T09:38:22.788Z"
   },
   {
    "duration": 48,
    "start_time": "2023-10-18T09:38:24.715Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-18T09:38:27.467Z"
   },
   {
    "duration": 59,
    "start_time": "2023-10-18T09:38:31.430Z"
   },
   {
    "duration": 5,
    "start_time": "2023-10-18T09:38:54.748Z"
   },
   {
    "duration": 60240,
    "start_time": "2023-10-18T09:39:01.968Z"
   },
   {
    "duration": 12569,
    "start_time": "2023-10-18T09:40:25.599Z"
   },
   {
    "duration": 7348,
    "start_time": "2023-10-18T09:40:42.506Z"
   },
   {
    "duration": 4009,
    "start_time": "2023-10-18T09:40:53.670Z"
   },
   {
    "duration": 5812,
    "start_time": "2023-10-18T09:41:01.167Z"
   },
   {
    "duration": 3446,
    "start_time": "2023-10-18T09:41:29.048Z"
   },
   {
    "duration": 808,
    "start_time": "2023-10-18T09:41:32.496Z"
   },
   {
    "duration": 17,
    "start_time": "2023-10-18T09:41:33.305Z"
   },
   {
    "duration": 25,
    "start_time": "2023-10-18T09:41:33.324Z"
   },
   {
    "duration": 18,
    "start_time": "2023-10-18T09:41:33.351Z"
   },
   {
    "duration": 46,
    "start_time": "2023-10-18T09:41:33.370Z"
   },
   {
    "duration": 6,
    "start_time": "2023-10-18T09:41:33.418Z"
   },
   {
    "duration": 7,
    "start_time": "2023-10-18T09:41:33.425Z"
   },
   {
    "duration": 12,
    "start_time": "2023-10-18T09:41:33.434Z"
   },
   {
    "duration": 27,
    "start_time": "2023-10-18T09:41:33.447Z"
   },
   {
    "duration": 34,
    "start_time": "2023-10-18T09:41:33.475Z"
   },
   {
    "duration": 7,
    "start_time": "2023-10-18T09:41:33.511Z"
   },
   {
    "duration": 32,
    "start_time": "2023-10-18T09:41:33.520Z"
   },
   {
    "duration": 27,
    "start_time": "2023-10-18T09:41:33.554Z"
   },
   {
    "duration": 46,
    "start_time": "2023-10-18T09:41:33.582Z"
   },
   {
    "duration": 54,
    "start_time": "2023-10-18T09:41:33.630Z"
   },
   {
    "duration": 2,
    "start_time": "2023-10-18T09:41:33.686Z"
   },
   {
    "duration": 9168270,
    "start_time": "2023-10-18T09:41:33.690Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T12:14:21.962Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T12:14:21.963Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T12:14:21.964Z"
   },
   {
    "duration": 2833,
    "start_time": "2023-10-18T16:52:04.463Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.299Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.300Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.301Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.303Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.303Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.305Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.306Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.307Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.308Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.309Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.310Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.311Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.312Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.314Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.314Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.316Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.317Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.318Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.320Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.321Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.323Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.324Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.325Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.344Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.346Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.347Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.348Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.350Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-18T16:52:07.351Z"
   },
   {
    "duration": 16691,
    "start_time": "2023-10-22T06:51:56.980Z"
   },
   {
    "duration": 1154,
    "start_time": "2023-10-22T06:52:13.673Z"
   },
   {
    "duration": 300,
    "start_time": "2023-10-22T06:52:14.828Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.130Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.131Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.132Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.133Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.134Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.135Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.136Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.137Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.138Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.139Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.139Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.141Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.141Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.142Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.143Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.144Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.146Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.147Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.147Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.149Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.150Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.151Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.152Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.153Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.154Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.154Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.155Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.156Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.157Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.158Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.159Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.160Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.161Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.224Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.226Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.228Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.229Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.230Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.232Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-22T06:52:15.233Z"
   }
  ],
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "339px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "256.844px",
    "left": "1485px",
    "right": "20px",
    "top": "122px",
    "width": "322px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
